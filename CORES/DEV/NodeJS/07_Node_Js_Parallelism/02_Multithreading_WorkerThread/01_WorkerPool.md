Iâ€™ll go in this order:

1. **Why worker pools exist**
2. **Mental model (very important)**
3. **Minimal worker pool implementation**
4. **Using the pool in a web server**
5. **Design intent & scaling rules**
6. **Common mistakes (interview + real world)**
    

No fluff.

---

## 1. Why Worker Pools Exist

### Problem with â€œnew Worker per requestâ€

- Worker startup is **expensive**
- Memory overhead per worker
- OS thread thrashing
- Under load â†’ **worse performance than single-threaded**
    

### Solution

- Create **N workers once**
- Reuse them
- Queue jobs
- Dispatch work when a worker is free
    

This is **exactly** how thread pools work in Java, Go, C++.

---

## 2. Worker Pool Mental Model

```js
Main Thread
 â”œâ”€ HTTP Server
 â”œâ”€ Job Queue
 â”œâ”€ Worker Pool (fixed size)
 â”‚   â”œâ”€ Worker #1 (busy / idle)
 â”‚   â”œâ”€ Worker #2
 â”‚   â””â”€ Worker #N
 â””â”€ Dispatcher

```

### Flow

1. Request arrives
2. Job pushed into queue
3. Free worker picks job
4. Worker sends result
5. Worker marked idle
6. Next job dispatched
    

**Key invariant:**  
ğŸ‘‰ _Number of workers never exceeds CPU cores_

---

## 3. Worker Code (Reusable Worker)

### `worker.js`

```js
const { parentPort } = require("worker_threads");

function heavyTask(n) {
  let result = 0;
  for (let i = 0; i < n; i++) {
    result += Math.sqrt(i);
  }
  return result;
}

parentPort.on("message", ({ taskId, data }) => {
  const result = heavyTask(data);
  parentPort.postMessage({ taskId, result });
});

```

### Why this structure

- Worker stays **alive**
- Handles **multiple tasks**
- Uses `taskId` to match responses
    

This is critical.

---

## 4. Worker Pool Implementation (From Scratch)

### `workerPool.js`

```js
const { Worker } = require("worker_threads");
const os = require("os");

class WorkerPool {
  constructor(workerPath, size = os.cpus().length) {
    this.workerPath = workerPath;
    this.size = size;
    this.workers = [];
    this.freeWorkers = [];
    this.queue = [];
    this.taskId = 0;

    for (let i = 0; i < size; i++) {
      this.createWorker();
    }
  }

  createWorker() {
    const worker = new Worker(this.workerPath);

    worker.on("message", (msg) => {
      worker.currentTask.resolve(msg.result);
      worker.currentTask = null;
      this.freeWorkers.push(worker);
      this.runNext();
    });

    worker.on("error", (err) => {
      worker.currentTask.reject(err);
    });

    this.workers.push(worker);
    this.freeWorkers.push(worker);
  }

  runTask(data) {
    return new Promise((resolve, reject) => {
      const task = {
        taskId: ++this.taskId,
        data,
        resolve,
        reject,
      };

      this.queue.push(task);
      this.runNext();
    });
  }

  runNext() {
    if (this.queue.length === 0) return;
    if (this.freeWorkers.length === 0) return;

    const worker = this.freeWorkers.pop();
    const task = this.queue.shift();

    worker.currentTask = task;
    worker.postMessage({
      taskId: task.taskId,
      data: task.data,
    });
  }
}

module.exports = WorkerPool;

```

---

## 5. Using the Pool in a Web Server

### `server.js`

```js
const http = require("http");
const WorkerPool = require("./workerPool");

const pool = new WorkerPool("./worker.js", 4);

const server = http.createServer(async (req, res) => {
  if (req.url === "/compute") {
    const result = await pool.runTask(40_000_000);
    res.end(`Result: ${result}`);
  } else {
    res.end("OK");
  }
});

server.listen(3000, () => {
  console.log("Server running on port 3000");
});

```

### What this achieves

- Fixed number of workers
- Requests donâ€™t spawn threads
- CPU usage stays predictable
- Event loop stays free
    

This is **production-safe**.

---

## 6. Context, Intent & Design Overview

### Context

- Nodeâ€™s event loop must stay responsive
- CPU-heavy work must run in parallel
- Threads are scarce resources
    

### Intent

- Control concurrency explicitly
- Prevent resource exhaustion
- Maximize CPU utilization
    

### Design Principles

1. **Fixed pool size**
2. **Queue backpressure**
3. **Workers are long-lived**
4. **Message-based protocol**
    

This is the same design used in:

- JVM thread pools
- Go worker pools
- OS schedulers
    

---

## 7. Pool Size: How Many Workers?

Rule of thumb:

```js
CPU-bound tasks â†’ number of CPU cores
I/O inside worker â†’ cores - 1

```

Never:

```js
pool size = request count âŒ

```

---

## 8. Common Mistakes (Very Important)

âŒ Spawning worker per request  
âŒ Passing huge objects (copy cost)  
âŒ No task IDs  
âŒ No backpressure handling  
âŒ Forgetting worker crashes

---

## 9. Interview-Ready Explanation (Memorize)

> â€œNode is single-threaded for execution.  
> For CPU-bound workloads, I use worker threads with a fixed-size pool.  
> The main thread acts as a scheduler, queues tasks, and dispatches them to idle workers.  
> This keeps the event loop responsive and ensures predictable CPU usage.â€

If you say this clearly, **you pass**.